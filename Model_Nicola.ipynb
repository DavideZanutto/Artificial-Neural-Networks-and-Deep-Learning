{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "id": "KNZotXHOapW8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1668429891880,
     "user_tz": -60,
     "elapsed": 4979,
     "user": {
      "displayName": "Nicola Cecere",
      "userId": "15930624256794819808"
     }
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from keras.regularizers import l2\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# Random seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)"
   ],
   "metadata": {
    "id": "CvMKuJH0apW-",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1668429894425,
     "user_tz": -60,
     "elapsed": 308,
     "user": {
      "displayName": "Nicola Cecere",
      "userId": "15930624256794819808"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "tf.get_logger().setLevel('INFO')\n",
    "tf.autograph.set_verbosity(0)\n",
    "\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ],
   "metadata": {
    "id": "bQIJQdcCapW_",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1668429898793,
     "user_tz": -60,
     "elapsed": 214,
     "user": {
      "displayName": "Nicola Cecere",
      "userId": "15930624256794819808"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data import"
   ],
   "metadata": {
    "collapsed": false,
    "id": "OSFbxo0DapXA"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "dataset_dir = '/Users/nicolacecere/GitHub/Artificial-Neural-Networks-and-Deep-Learning/SplitData/data'\n",
    "training_dir = '/Users/nicolacecere/GitHub/Artificial-Neural-Networks-and-Deep-Learning/SplitData/data/training'\n",
    "validation_dir = '/Users/nicolacecere/GitHub/Artificial-Neural-Networks-and-Deep-Learning/SplitData/data/validation'\n",
    "test_dir = '/Users/nicolacecere/GitHub/Artificial-Neural-Networks-and-Deep-Learning/SplitData/data/test'"
   ],
   "metadata": {
    "id": "Jq6Y5CcYapXA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1668430262602,
     "user_tz": -60,
     "elapsed": 2,
     "user": {
      "displayName": "Nicola Cecere",
      "userId": "15930624256794819808"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2835 images belonging to 8 classes.\n",
      "Found 398 images belonging to 8 classes.\n",
      "Found 495 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "# Images are divided into folders, one for each class.\n",
    "# If the images are organized in such a way, we can exploit the\n",
    "# ImageDataGenerator to read them from disk.\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create an instance of ImageDataGenerator for training, validation, and test sets\n",
    "train_data_gen = ImageDataGenerator(rescale = 1./255)\n",
    "valid_data_gen = ImageDataGenerator(rescale = 1./255)\n",
    "test_data_gen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# Obtain a data generator with the 'ImageDataGenerator.flow_from_directory' method\n",
    "train_gen = train_data_gen.flow_from_directory(directory=training_dir,\n",
    "                                               target_size=(96,96),\n",
    "                                               color_mode='rgb',\n",
    "                                               classes=None, # can be set to labels\n",
    "                                               class_mode='categorical',\n",
    "                                               batch_size=32,\n",
    "                                               shuffle=True,\n",
    "                                               seed=seed\n",
    "                                               )\n",
    "valid_gen = train_data_gen.flow_from_directory(directory=validation_dir,\n",
    "                                               target_size=(96,96),\n",
    "                                               color_mode='rgb',\n",
    "                                               classes=None, # can be set to labels\n",
    "                                               class_mode='categorical',\n",
    "                                               batch_size=32,\n",
    "                                               shuffle=True,\n",
    "                                               seed=seed)\n",
    "test_gen = train_data_gen.flow_from_directory(directory=test_dir,\n",
    "                                              target_size=(96,96),\n",
    "                                              color_mode='rgb',\n",
    "                                              classes=None, # can be set to labels\n",
    "                                              class_mode='categorical',\n",
    "                                              batch_size=32,\n",
    "                                              shuffle=False,\n",
    "                                              seed=seed)"
   ],
   "metadata": {
    "id": "Jbjo4RcvapXC",
    "outputId": "147f6090-320c-4954-e997-0acf1aee2fd5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1668430272236,
     "user_tz": -60,
     "elapsed": 6763,
     "user": {
      "displayName": "Nicola Cecere",
      "userId": "15930624256794819808"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Augmentation"
   ],
   "metadata": {
    "collapsed": false,
    "id": "JVZpBO_bapXD"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "<keras.preprocessing.image.ImageDataGenerator at 0x7fc543499360>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    featurewise_center=False, samplewise_center=False,\n",
    "    featurewise_std_normalization=False, samplewise_std_normalization=False,\n",
    "    zca_whitening=False, zca_epsilon=1e-06, rotation_range=0, width_shift_range=0.0,\n",
    "    height_shift_range=0.0, brightness_range=None, shear_range=0.0, zoom_range=0.0,\n",
    "    channel_shift_range=0.0, fill_mode='nearest', cval=0.0,\n",
    "    horizontal_flip=False, vertical_flip=False, rescale=None,\n",
    "    preprocessing_function=None, data_format=None, validation_split=0.0, dtype=None\n",
    ")"
   ],
   "metadata": {
    "id": "6YrWUsdmapXE",
    "outputId": "7d3549f0-1f80-4cbd-fa12-d7f941a79b80",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1668430277948,
     "user_tz": -60,
     "elapsed": 232,
     "user": {
      "displayName": "Nicola Cecere",
      "userId": "15930624256794819808"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2835 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of ImageDataGenerator with Data Augmentation\n",
    "aug_train_data_gen = ImageDataGenerator(rotation_range=20,\n",
    "                                        horizontal_flip= True,\n",
    "                                        vertical_flip= True,\n",
    "                                        brightness_range=(0.6,1.4),\n",
    "                                        zoom_range=0.6,\n",
    "                                        fill_mode='nearest',\n",
    "                                        rescale=1./255) # rescale value is multiplied to the image\n",
    "\n",
    "# Obtain a data generator with the 'ImageDataGenerator.flow_from_directory' method\n",
    "aug_train_gen = aug_train_data_gen.flow_from_directory(directory=training_dir,\n",
    "                                                       target_size=(96,96),\n",
    "                                                       color_mode='rgb',\n",
    "                                                       classes=None, # can be set to labels\n",
    "                                                       class_mode='categorical',\n",
    "                                                       batch_size=32,\n",
    "                                                       shuffle=True,\n",
    "                                                       seed=seed)"
   ],
   "metadata": {
    "id": "ilLddj5LapXG",
    "outputId": "e2f49ece-7c0b-446c-e1a1-2ab420d4b4eb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1668430279500,
     "user_tz": -60,
     "elapsed": 6,
     "user": {
      "displayName": "Nicola Cecere",
      "userId": "15930624256794819808"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false,
    "id": "4Gc1EGSbapXG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "input_shape = (96, 96, 3)\n",
    "epochs = 200"
   ],
   "metadata": {
    "id": "lvv6irhHapXH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1668430281738,
     "user_tz": -60,
     "elapsed": 1,
     "user": {
      "displayName": "Nicola Cecere",
      "userId": "15930624256794819808"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "il primo modello è il migliore fin ora -> provare a migliorare con le cose del terzo modello"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "\n",
    "    # Build the neural network layer by layer\n",
    "    input_layer = tfkl.Input(shape=input_shape, name='input_layer')\n",
    "\n",
    "    conv1 = tfkl.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        padding = 'same',\n",
    "        activation = 'relu',\n",
    "        kernel_initializer = tfk.initializers.HeUniform(seed)\n",
    "    )(input_layer)\n",
    "    pool1 = tfkl.MaxPooling2D()(conv1)\n",
    "\n",
    "    conv2 = tfkl.Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=3,\n",
    "        padding = 'same',\n",
    "        activation = 'relu',\n",
    "        kernel_initializer = tfk.initializers.HeUniform(seed)\n",
    "    )(pool1)\n",
    "    pool2 = tfkl.MaxPooling2D()(conv2)\n",
    "\n",
    "    conv3 = tfkl.Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=3,\n",
    "        padding = 'same',\n",
    "        activation = 'relu',\n",
    "        kernel_initializer = tfk.initializers.HeUniform(seed)\n",
    "    )(pool2)\n",
    "    conv4 = tfkl.Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=3,\n",
    "        padding = 'same',\n",
    "        activation = 'relu',\n",
    "        kernel_initializer = tfk.initializers.HeUniform(seed)\n",
    "    )(conv3)\n",
    "    pool3 = tfkl.MaxPooling2D()(conv4)\n",
    "\n",
    "    conv5 = tfkl.Conv2D(\n",
    "        filters=256,\n",
    "        kernel_size=3,\n",
    "        padding = 'same',\n",
    "        activation = 'relu',\n",
    "        kernel_initializer = tfk.initializers.HeUniform(seed)\n",
    "    )(pool3)\n",
    "    pool4 = tfkl.MaxPooling2D()(conv5)\n",
    "\n",
    "    conv6 = tfkl.Conv2D(\n",
    "        filters=256,\n",
    "        kernel_size=3,\n",
    "        padding = 'same',\n",
    "        activation = 'relu',\n",
    "        kernel_initializer = tfk.initializers.HeUniform(seed)\n",
    "    )(pool4)\n",
    "    pool5 = tfkl.GlobalAveragePooling2D(name='gap')(conv6)\n",
    "\n",
    "    flattening_layer = tfkl.Flatten(name='Flatten')(pool5)\n",
    "    dropout = tfkl.Dropout(0.2, seed=seed)(flattening_layer)\n",
    "    classifier_layer = tfkl.Dense(units=256, name='Classifier', kernel_initializer=tfk.initializers.HeUniform(seed), activation='relu')(dropout)\n",
    "    dropout = tfkl.Dropout(0.2, seed=seed)(classifier_layer)\n",
    "    output_layer = tfkl.Dense(units=8, activation='softmax', kernel_initializer=tfk.initializers.GlorotUniform(seed), name='output_layer')(dropout)\n",
    "\n",
    "    # Connect input and output through the Model class\n",
    "    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n",
    "\n",
    "    # Return the model\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    # Build the neural network layer by layer\n",
    "    input_layer = tfkl.Input(shape=input_shape, name='input_layer')\n",
    "\n",
    "    x = tfkl.Conv2D(\n",
    "        filters = 64,\n",
    "        kernel_size = 3,\n",
    "        padding = 'same',\n",
    "        activation = 'relu',\n",
    "        kernel_initializer = tfk.initializers.HeUniform(seed),\n",
    "        name = 'conv1')(input_layer)\n",
    "    x = tfkl.MaxPooling2D(name='mp1')(x)\n",
    "\n",
    "    x = tfkl.Conv2D(\n",
    "        filters = 128,\n",
    "        kernel_size = 3,\n",
    "        padding = 'same',\n",
    "        activation = 'relu',\n",
    "        kernel_initializer = tfk.initializers.HeUniform(seed),\n",
    "        name = 'conv2')(x)\n",
    "    x = tfkl.MaxPooling2D(name='mp2')(x)\n",
    "\n",
    "    x = tfkl.Conv2D(\n",
    "        filters = 256,\n",
    "        kernel_size = 3,\n",
    "        padding = 'same',\n",
    "        activation = 'relu',\n",
    "        kernel_initializer = tfk.initializers.HeUniform(seed),\n",
    "        name = 'Conv3')(x)\n",
    "    x = tfkl.GlobalAveragePooling2D(name='gap')(x) #different pooling function -> some advantages like explainability\n",
    "    x = tfkl.Dropout(0.3, seed=seed, name='gap_dropout')(x)\n",
    "\n",
    "    x = tfkl.Dense(\n",
    "        units = 256,\n",
    "        activation = 'relu',\n",
    "        kernel_initializer = tfk.initializers.GlorotUniform(seed),\n",
    "        name = 'classifier')(x)\n",
    "    x = tfkl.Dropout(0.3, seed=seed, name='classifier_dropout')(x)\n",
    "\n",
    "    output_layer = tfkl.Dense(\n",
    "        units = 8,\n",
    "        activation = 'softmax',\n",
    "        kernel_initializer = tfk.initializers.GlorotUniform(seed),\n",
    "        name = 'output_layer')(x)\n",
    "\n",
    "    # Connect input and output through the Model class\n",
    "    model = tfk.Model(inputs = input_layer, outputs = output_layer, name = 'model')\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n",
    "\n",
    "    # Return the model\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "\n",
    "\n",
    "    # Layer Input -------------------------------------------------------\n",
    "    input_layer = tfkl.Input(shape=input_shape, name='Input')\n",
    "\n",
    "\n",
    "\n",
    "    # Layer 1 -----------------------------------------------------------\n",
    "    conv1 = tfkl.Conv2D(\n",
    "        filters=25,\n",
    "        kernel_size=(3, 3),\n",
    "        strides = (1, 1),\n",
    "        padding = 'same',\n",
    "        kernel_initializer = tfk.initializers.GlorotUniform(seed),\n",
    "        kernel_regularizer = l2(0.01)\n",
    "    )(input_layer)\n",
    "\n",
    "    conv1 = tfkl.BatchNormalization()(conv1)\n",
    "\n",
    "    leaky_relu_layer1 = tfkl.LeakyReLU()(conv1)\n",
    "\n",
    "    pool1 = tfkl.MaxPooling2D(\n",
    "        pool_size = (2, 2)\n",
    "    )(leaky_relu_layer1)\n",
    "\n",
    "\n",
    "\n",
    "    # Layer 2 -----------------------------------------------------------\n",
    "    conv2 = tfkl.Conv2D(\n",
    "        filters=50,\n",
    "        kernel_size=(3, 3),\n",
    "        strides = (1, 1),\n",
    "        padding = 'same',\n",
    "        kernel_initializer = tfk.initializers.GlorotUniform(seed),\n",
    "        kernel_regularizer = l2(0.01)\n",
    "    )(pool1)\n",
    "\n",
    "    conv2 = tfkl.BatchNormalization()(conv2)\n",
    "\n",
    "    leaky_relu_layer2 = tfkl.LeakyReLU()(conv2)\n",
    "\n",
    "    pool2 = tfkl.MaxPooling2D(\n",
    "        pool_size = (2, 2)\n",
    "    )(leaky_relu_layer2)\n",
    "\n",
    "\n",
    "\n",
    "    # Layer 3 -----------------------------------------------------------\n",
    "    conv3 = tfkl.Conv2D(\n",
    "        filters=100,\n",
    "        kernel_size=(3, 3),\n",
    "        strides = (1, 1),\n",
    "        padding = 'same',\n",
    "        kernel_initializer = tfk.initializers.GlorotUniform(seed),\n",
    "        kernel_regularizer = l2(0.01)\n",
    "    )(pool2)\n",
    "\n",
    "    conv3 = tfkl.BatchNormalization()(conv3)\n",
    "\n",
    "    leaky_relu_layer3 = tfkl.LeakyReLU()(conv3)\n",
    "\n",
    "    pool3 = tfkl.MaxPooling2D(\n",
    "        pool_size = (2, 2)\n",
    "    )(leaky_relu_layer3)\n",
    "\n",
    "\n",
    "\n",
    "    # Layer 4 -----------------------------------------------------------\n",
    "    conv4 = tfkl.Conv2D(\n",
    "        filters=200,\n",
    "        kernel_size=(5, 5),\n",
    "        strides = (1, 1),\n",
    "        padding = 'same',\n",
    "        kernel_initializer = tfk.initializers.GlorotUniform(seed),\n",
    "        kernel_regularizer = l2(0.01)\n",
    "    )(pool3)\n",
    "\n",
    "    conv4 = tfkl.BatchNormalization()(conv4)\n",
    "\n",
    "    leaky_relu_layer4 = tfkl.LeakyReLU()(conv4)\n",
    "\n",
    "    pool4 = tfkl.MaxPooling2D(\n",
    "        pool_size = (2, 2)\n",
    "    )(leaky_relu_layer4)\n",
    "\n",
    "\n",
    "\n",
    "    # Layer 5 -----------------------------------------------------------\n",
    "    conv5 = tfkl.Conv2D(\n",
    "        filters=300,\n",
    "        kernel_size=(5, 5),\n",
    "        strides = (1, 1),\n",
    "        padding = 'same',\n",
    "        kernel_initializer = tfk.initializers.GlorotUniform(seed),\n",
    "        kernel_regularizer = l2(0.01)\n",
    "    )(pool4)\n",
    "\n",
    "    conv5 = tfkl.BatchNormalization()(conv5)\n",
    "\n",
    "    leaky_relu_layer5 = tfkl.LeakyReLU()(conv5)\n",
    "\n",
    "    pool5 = tfkl.MaxPooling2D(\n",
    "        pool_size = (2, 2)\n",
    "    )(leaky_relu_layer5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Global Average Pooling -----------------------------------------------------------\n",
    "    glob_pooling = tfkl.GlobalAveragePooling2D(name='GlobalPooling')(leaky_relu_layer5)\n",
    "\n",
    "\n",
    "    # Dense Layer -----------------------------------------------------------\n",
    "    classifier_layer1 = tfkl.Dense(units=512, name='Classifier1', kernel_initializer=tfk.initializers.GlorotUniform(seed), kernel_regularizer = l2(0.01))(glob_pooling)\n",
    "\n",
    "    classifier_layer1 = tfkl.BatchNormalization()(classifier_layer1)\n",
    "\n",
    "    leaky_relu_layer = tfkl.LeakyReLU()(classifier_layer1)\n",
    "\n",
    "    leaky_relu_layer = tfkl.Dropout(0.3, seed=seed)(leaky_relu_layer)\n",
    "\n",
    "\n",
    "    # Output Layer -----------------------------------------------------------\n",
    "    output_layer = tfkl.Dense(units=8, activation='softmax', kernel_initializer=tfk.initializers.GlorotUniform(seed), name='Output')(leaky_relu_layer)\n",
    "\n",
    "\n",
    "    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n",
    "\n",
    "    model.compile(\n",
    "        loss=tfk.losses.CategoricalCrossentropy(),\n",
    "        optimizer=tfk.optimizers.Adam(),\n",
    "        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "    )\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def create_folders_and_callbacks(model_name):\n",
    "\n",
    "  exps_dir = os.path.join('data_augmentation_experiments')\n",
    "  if not os.path.exists(exps_dir):\n",
    "      os.makedirs(exps_dir)\n",
    "\n",
    "  now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "\n",
    "  exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n",
    "  if not os.path.exists(exp_dir):\n",
    "      os.makedirs(exp_dir)\n",
    "\n",
    "  callbacks = []\n",
    "\n",
    "  # Model checkpoint\n",
    "  # ----------------\n",
    "  ckpt_dir = os.path.join(exp_dir, 'ckpts')\n",
    "  if not os.path.exists(ckpt_dir):\n",
    "      os.makedirs(ckpt_dir)\n",
    "\n",
    "  ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp.ckpt'),\n",
    "                                                     save_weights_only=True, # True to save only weights\n",
    "                                                     save_best_only=False) # True to save only the best epoch\n",
    "  callbacks.append(ckpt_callback)\n",
    "\n",
    "  # Visualize Learning on Tensorboard\n",
    "  # ---------------------------------\n",
    "  tb_dir = os.path.join(exp_dir, 'tb_logs')\n",
    "  if not os.path.exists(tb_dir):\n",
    "      os.makedirs(tb_dir)\n",
    "\n",
    "  # By default shows losses and metrics for both training and validation\n",
    "  tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n",
    "                                               profile_batch=0,\n",
    "                                               histogram_freq=1)  # if > 0 (epochs) shows weights histograms\n",
    "  callbacks.append(tb_callback)\n",
    "\n",
    "  # Early Stopping\n",
    "  # --------------\n",
    "  es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "  callbacks.append(es_callback)\n",
    "\n",
    "  return callbacks"
   ],
   "metadata": {
    "id": "QgDzFFTUapXH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1668430285936,
     "user_tz": -60,
     "elapsed": 204,
     "user": {
      "displayName": "Nicola Cecere",
      "userId": "15930624256794819808"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 96, 96, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 96, 96, 25)        700       \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 96, 96, 25)       100       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_14 (LeakyReLU)  (None, 96, 96, 25)        0         \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 48, 48, 25)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 48, 48, 50)        11300     \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 48, 48, 50)       200       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_15 (LeakyReLU)  (None, 48, 48, 50)        0         \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 24, 24, 50)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 24, 24, 100)       45100     \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 24, 24, 100)      400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_16 (LeakyReLU)  (None, 24, 24, 100)       0         \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPoolin  (None, 12, 12, 100)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 12, 12, 200)       500200    \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 12, 12, 200)      800       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_17 (LeakyReLU)  (None, 12, 12, 200)       0         \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPoolin  (None, 6, 6, 200)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 6, 6, 300)         1500300   \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 6, 6, 300)        1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_18 (LeakyReLU)  (None, 6, 6, 300)         0         \n",
      "                                                                 \n",
      " GlobalPooling (GlobalAverag  (None, 300)              0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " Classifier1 (Dense)         (None, 512)               154112    \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 512)              2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_19 (LeakyReLU)  (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " Output (Dense)              (None, 8)                 4104      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,220,564\n",
      "Trainable params: 2,218,190\n",
      "Non-trainable params: 2,374\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build model (for data augmentation training)\n",
    "model = build_model(input_shape)\n",
    "model.summary()"
   ],
   "metadata": {
    "id": "FEkYSrT2apXI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1668430289188,
     "user_tz": -60,
     "elapsed": 526,
     "user": {
      "displayName": "Nicola Cecere",
      "userId": "15930624256794819808"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "89/89 [==============================] - 36s 386ms/step - loss: 8.5927 - accuracy: 0.2998 - precision_2: 0.3877 - recall_2: 0.1249 - val_loss: 8.2485 - val_accuracy: 0.1608 - val_precision_2: 0.1608 - val_recall_2: 0.1608\n",
      "Epoch 2/200\n",
      "89/89 [==============================] - 32s 362ms/step - loss: 4.7843 - accuracy: 0.3351 - precision_2: 0.4125 - recall_2: 0.1464 - val_loss: 6.1621 - val_accuracy: 0.1608 - val_precision_2: 0.1608 - val_recall_2: 0.1608\n",
      "Epoch 3/200\n",
      "89/89 [==============================] - 30s 341ms/step - loss: 3.3071 - accuracy: 0.3654 - precision_2: 0.4581 - recall_2: 0.1714 - val_loss: 3.4624 - val_accuracy: 0.1910 - val_precision_2: 0.3000 - val_recall_2: 0.0075\n",
      "Epoch 4/200\n",
      "89/89 [==============================] - 33s 365ms/step - loss: 2.6685 - accuracy: 0.3753 - precision_2: 0.4708 - recall_2: 0.1788 - val_loss: 2.7425 - val_accuracy: 0.3065 - val_precision_2: 0.2264 - val_recall_2: 0.0603\n",
      "Epoch 5/200\n",
      "89/89 [==============================] - 32s 353ms/step - loss: 2.3464 - accuracy: 0.3757 - precision_2: 0.4717 - recall_2: 0.1644 - val_loss: 2.3542 - val_accuracy: 0.3643 - val_precision_2: 0.6545 - val_recall_2: 0.0905\n",
      "Epoch 6/200\n",
      "89/89 [==============================] - 33s 374ms/step - loss: 2.1133 - accuracy: 0.3972 - precision_2: 0.5224 - recall_2: 0.2014 - val_loss: 2.5357 - val_accuracy: 0.2487 - val_precision_2: 0.3925 - val_recall_2: 0.1055\n",
      "Epoch 7/200\n",
      "89/89 [==============================] - 32s 364ms/step - loss: 1.9889 - accuracy: 0.4205 - precision_2: 0.5268 - recall_2: 0.2222 - val_loss: 2.4941 - val_accuracy: 0.1935 - val_precision_2: 0.2761 - val_recall_2: 0.1131\n",
      "Epoch 8/200\n",
      "89/89 [==============================] - 32s 362ms/step - loss: 1.9217 - accuracy: 0.4257 - precision_2: 0.5472 - recall_2: 0.2148 - val_loss: 2.1121 - val_accuracy: 0.3794 - val_precision_2: 0.4317 - val_recall_2: 0.1985\n",
      "Epoch 9/200\n",
      "89/89 [==============================] - 32s 358ms/step - loss: 1.8818 - accuracy: 0.4332 - precision_2: 0.5538 - recall_2: 0.2342 - val_loss: 2.6143 - val_accuracy: 0.3015 - val_precision_2: 0.5000 - val_recall_2: 0.0779\n",
      "Epoch 10/200\n",
      "89/89 [==============================] - 36s 401ms/step - loss: 1.8977 - accuracy: 0.4229 - precision_2: 0.5485 - recall_2: 0.2155 - val_loss: 2.1670 - val_accuracy: 0.3568 - val_precision_2: 0.5680 - val_recall_2: 0.1784\n",
      "Epoch 11/200\n",
      "89/89 [==============================] - 32s 361ms/step - loss: 1.8912 - accuracy: 0.4152 - precision_2: 0.5443 - recall_2: 0.2123 - val_loss: 2.4771 - val_accuracy: 0.1432 - val_precision_2: 0.1595 - val_recall_2: 0.0653\n",
      "Epoch 12/200\n",
      "89/89 [==============================] - 32s 362ms/step - loss: 1.8408 - accuracy: 0.4363 - precision_2: 0.5693 - recall_2: 0.2303 - val_loss: 2.4045 - val_accuracy: 0.2513 - val_precision_2: 0.2500 - val_recall_2: 0.0226\n",
      "Epoch 13/200\n",
      "89/89 [==============================] - 32s 355ms/step - loss: 1.8063 - accuracy: 0.4392 - precision_2: 0.5815 - recall_2: 0.2367 - val_loss: 2.0064 - val_accuracy: 0.3367 - val_precision_2: 0.5619 - val_recall_2: 0.1482\n",
      "Epoch 14/200\n",
      "89/89 [==============================] - 33s 372ms/step - loss: 1.8149 - accuracy: 0.4388 - precision_2: 0.5594 - recall_2: 0.2293 - val_loss: 2.3745 - val_accuracy: 0.2839 - val_precision_2: 0.3874 - val_recall_2: 0.2161\n",
      "Epoch 15/200\n",
      "89/89 [==============================] - 29s 330ms/step - loss: 1.7875 - accuracy: 0.4522 - precision_2: 0.6003 - recall_2: 0.2469 - val_loss: 2.0696 - val_accuracy: 0.3593 - val_precision_2: 0.3711 - val_recall_2: 0.1809\n",
      "Epoch 16/200\n",
      "89/89 [==============================] - 29s 326ms/step - loss: 1.7881 - accuracy: 0.4356 - precision_2: 0.5869 - recall_2: 0.2240 - val_loss: 2.3224 - val_accuracy: 0.1985 - val_precision_2: 0.1925 - val_recall_2: 0.0779\n",
      "Epoch 17/200\n",
      "89/89 [==============================] - 29s 324ms/step - loss: 1.7552 - accuracy: 0.4325 - precision_2: 0.5650 - recall_2: 0.2131 - val_loss: 1.8677 - val_accuracy: 0.3744 - val_precision_2: 0.8000 - val_recall_2: 0.1608\n",
      "Epoch 18/200\n",
      "89/89 [==============================] - 29s 326ms/step - loss: 1.7552 - accuracy: 0.4501 - precision_2: 0.5755 - recall_2: 0.2325 - val_loss: 1.9824 - val_accuracy: 0.2839 - val_precision_2: 0.4122 - val_recall_2: 0.1533\n"
     ]
    }
   ],
   "source": [
    "# Create folders and callbacks and fit\n",
    "aug_callbacks = create_folders_and_callbacks(model_name='CNN_Aug')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x = aug_train_gen,\n",
    "    epochs = epochs,\n",
    "    validation_data = valid_gen,\n",
    "    callbacks = aug_callbacks,\n",
    "    batch_size=32\n",
    ").history"
   ],
   "metadata": {
    "id": "EUdsjUfdapXI",
    "outputId": "be5340c2-af75-4003-8380-169b2370318e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1668431787144,
     "user_tz": -60,
     "elapsed": 1492245,
     "user": {
      "displayName": "Nicola Cecere",
      "userId": "15930624256794819808"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "# Save best epoch model\n",
    "model.save(\"data_augmentation_experiments/CNN_Aug_Best\")"
   ],
   "metadata": {
    "id": "up5di_mcapXI",
    "outputId": "9828dc0d-172e-446b-97fe-86433966240e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1668431799360,
     "user_tz": -60,
     "elapsed": 3030,
     "user": {
      "displayName": "Nicola Cecere",
      "userId": "15930624256794819808"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 2s 86ms/step - loss: 2.0864 - accuracy: 0.3919 - precision_2: 0.4265 - recall_2: 0.1758\n",
      "\n",
      "Test metrics with data augmentation\n",
      "{'loss': 2.086407423019409, 'accuracy': 0.39191919565200806, 'precision_2': 0.4264705777168274, 'recall_2': 0.17575757205486298}\n"
     ]
    }
   ],
   "source": [
    "# Trained with data augmentation\n",
    "model_aug = tfk.models.load_model(\"data_augmentation_experiments/CNN_Aug_Best\")\n",
    "model_aug_test_metrics = model_aug.evaluate(test_gen, return_dict=True)\n",
    "\n",
    "print()\n",
    "print(\"Test metrics with data augmentation\")\n",
    "print(model_aug_test_metrics)\n"
   ],
   "metadata": {
    "id": "NYFv3JpWapXJ",
    "outputId": "7b05027c-379e-4d92-d399-a4cf6a691d31",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1668431881358,
     "user_tz": -60,
     "elapsed": 78439,
     "user": {
      "displayName": "Nicola Cecere",
      "userId": "15930624256794819808"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [
    {
     "file_id": "16TOaMkqrjqSOW8YTD4_cbVwwqEK7edNI",
     "timestamp": 1668428132632
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
